2023-02-02 @ 13:39
Rodríguez López, Alejandro // UO281827

Tags:
	#showable
	Hecho en #EPI
	Sobre #Sistemas_Inteligentes 
	Para #Apuntes 
	Otros: #Dataset #EvaluacionModelo
	Refs:
 
<hr>

# Evaluación
Dado un conjunto de datos de entrada, se puede entrenar un algoritmo para que resuelva las clases.
El principio de evaluación se basa en utilizar el algoritmo obtenido con un **nuevo** set de datos para comprobar si es correcto.

Dataset -> Machine Learning Algorithm -> Hypothesis
New input -> Hypothesis -> Predicted class

Si se utilizase el dataset original para hacer tests, entonces el método del vecino más próximo sería el mejor.
La mejor opción es dividir el dataset en un training set y un test set. Utilizar el training set para entrenar y el test set para probar.

# Evaluación del rendimiento
Métricas de rendimiento:
- Número de clasificaciones correctas.
- Precisión en las estimaciones.
- Error en predicciones numéricas.

Es posible también ponderar los fallos, de forma que unos valgan más que otros. (i.e. un error en clasificación puede ser más importante que uno en precisión).

# Parameter tuning
El método del vecino más próximo tiene un parámetro, el número de vecinos a consultar.
El conjunto de test no se debe usar para afinar los parámetros, se debería utilizar el conjunto de entrenamiento. Éste se divide en dos: entrenamiento y validación.
El conjunto de entrenamiento se utiliza para optimizar los parámetros.

# Final de evaluación
Es necesario entregar el modelo, código y función junto con una estimación de la fiabilidad.

- Test: Estima la fiabilidad del modelo.

# Holdout estimation
¿Qué sucede si el conjunto de datos es limitado? ¿Cómo se reparte entre entrenamiento y test?
Las muestras deben ser representativas, tanto en el test como en el set de entrenamiento.
Si en el dataset original había 3 clases representadas por las siguientes cantidades:
- A: 15%
- B: 50%
- C: 35%
Entonces, tanto el conjunto de test como el de entrenamiento deben tener las mismas proporciones:
- A: 15%
- B: 50%
- C: 35%

# Cross-Validation
Trata de reducir la varianza en los sets.
## n-fold cross-validation
1. Dividir el set en `n` partes estratificadas.
2. Extraer una de las partes para el test.
3. El resto de `n-1` partes se utilizan para entrenamiento.

Repetir con una parte distinta hasta que cada parte del test se haya utilizado una vez para test.
En ese punto, se cada parte se ha utilizado una vez para test y `n-1` veces para entrenamiento.
Para cada test se recoge el acierto del modelo.

Finalmente, set utiliza el 100% del dataset para entrenar y se entrega el modelo resultante junto con la media de los resultado de las `n` evaluaciones.

En la práctica, `n` suele tener el valor de 10 porque funciona.

## Leave-one-out cross-validation (LOO)
De los `n` ejemplos del dataset, se retira 1 y se entrena con los `n-1` restantes y se continúa con el procedimiento de n-fold cross-validation.
Desventaja: No es posible una versión estratificada del test porque sólo contiene un ejemplo.
Spoiler: No es recomendable


Lots of data ? percentage split : stratified n-fold cross-variation
