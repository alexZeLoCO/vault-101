<% tp.file.creation_date() %> (YYYY-MM-DD) @ 11:26
Rodríguez López, Alejandro // UO281827

Tags:
	#showable
	Hecho en #EPI
	Sobre #Sistemas_Inteligentes 
	Para #Apuntes 
	Otros:
	Refs:
 
<hr>

Deep Learning es un algoritmo de aprendizaje automático.

# The goal of deep learning
Look for the nonlinear function:
$$f: \mathbb{R}^m \rightarrow \mathbb{R}^n, f(x, \theta) = f_\theta(x)$$
With the greatest consistency with the data
$$D = \{(x, y) : x \rightsquigarrow y\}$$
What is meaning of consistency in this context?
- Ease estimations $y \approx \Psi(f(x, \theta))$
- Make minimum a loss function (inconsistency with the data)

## Loss function
$$\theta \leftarrow argmin_\theta\ {1 \over {|D|}} \sum_{(x, y) \in D} loss(y, f_\theta(x))$$
Example: Linear regression $(n=1)$ with a quadratic loss
$$
\begin{matrix}
loss(y, f_w(x)) &=& { 1 \over 2 } \cdot (y - f_w(x))^2 \\
f_w(x) &=& w \cdot x
\end{matrix}
$$
	
<hr>

2023-02-20 (YYYY-MM-DD) @ 10:13

Las soluciones al problema son no lineales. ==> La función de pérdida es no convexa.

```functionplot
---
title: Función Convexa
xLabel: 
yLabel: 
bounds: [-0.5,1,-0.1,1]
disableZoom: false
grid: true
---
f(x) = x^2
```
```functionplot
---
title: Función NO Convexa
xLabel: 
yLabel: 
bounds: [-10,10,-1.5,1.5]
disableZoom: false
grid: true
---
f(x) = sin(x) 
```

Es más fácil resolver el mínimo de una función convexa que el de una no convexa.

# Optimización
La optimización requiere sus derivadas. Buscamos minimizar la función:
$$\theta \leftarrow argmin_\theta\ {1 \over {|D|}} \sum_{(x, y) \in D} loss(y, f_\theta(x))$$
Donde $y$ es el resultado real, $x$ es la entrada al modelo y $f_{\theta}(x)$ es lo que retorna el modelo. Lo que retorna el modelo se calcula en función de $\theta$.
La función a optimizar es multivariable, las derivadas parciales se deben realizar en función de $\theta$

## Gradient Descent
Reescribimos la funcion:

$$\theta \leftarrow argmin_\theta\ {1 \over {|D|}} \sum_{(x, y) \in D} loss(y, f_\theta(x)) = \theta \leftarrow argmin_\theta\ J(\theta)$$

$$
\begin{matrix}
\theta \leftarrow \theta_0 \\
repeat\\
&\theta \leftarrow \theta - \gamma \cdot { \partial J \over \partial \theta } \\
until\ no\ more\ improvement\ can\ be\ made\\
return\ \theta
\end{matrix}
$$

# General framework
$$D = \{(x,y): x \rightsquigarrow y\}$$
We learn a distribution
$$Pr(y|x, \theta)$$
That involves the function
$$f: \mathbb{R}^m \rightarrow \mathbb{R}^n, f_\theta(x)$$
This requires the maximum likelihood
$$argmax_\theta \prod_D Pr(x|y,\theta)$$
O lo que es lo mismo:
$$argmin_\theta -log(\prod_D Pr(y|x,\theta)) = argmin_\theta - \sum_D log (Pr(y|x,\theta))$$
En muchos casos la operación anterior se resolverá a algo similar a: $log(e^x)$ que se resuelve trivialmente a $x$.

# Regresión
$$
\begin{matrix}
y \in \mathbb{R};\ D &=& \{(x, y),\ ...\} \\
Pr(y|x, \theta) &=& N(y;f(x, \theta), \sigma^2) \\
N(y;f(x, \theta), \sigma^2) &=& \sqrt{1 \over {2\cdot \pi \cdot \sigma^2}} \cdot e^{-{1 \over {2 \cdot \sigma^2}}\cdot (y-\mu)^2}\\
-log(Pr(y|x,\theta)) &=& log (\sigma) + {1 \over 2} \cdot log(2 \cdot \pi) + {1 \over {2 \cdot \sigma^2}} \cdot (y - f(x, \theta))^2 \\
argmin_\theta - log( \prod_D (Pr(y|x,\theta))) &=& argmin_\theta \sum_D (y-f(x, \theta))^2
\end{matrix}
$$

$$
Pr(y|x, \theta) \propto e^{y \cdot f(x, \theta)}
$$
Entonces,
$$
\begin{matrix}
Pr(y|x,\theta) &=& {{e^{y\cdot f(x, \theta)}}\over{\sum_{y'}e^{y' \cdot f(x, \theta)}}} \\
&=&{e^{y \cdot f(x, \theta)} \over {1+e^{f(x, \theta)}}}
\end{matrix}
$$

<hr>

$$
\begin{matrix}
\begin{bmatrix}
x_0 & x_1
\end{bmatrix}
\times
\begin{bmatrix}
w_0 \\
w_1
\end{bmatrix}
\cdot
bias = y
\end{matrix}
$$

Tras calcular $y$, se aplica la función de pérdida ($loss(y,\ y')$)y se modifica la matriz $w$ y el valor $bias$ para reducir la pérdida.
