2023-02-06 10:16 (YYYY-MM-DD) @ 10:16
Rodríguez López, Alejandro // UO281827

Tags:
	#showable
	Hecho en #EPI
	Sobre #Sistemas_Inteligentes 
	Para #Apuntes 
	Otros: #Hyperplanes #Subspaces
	Refs:
 
<hr>

# Feature maps
let
$$\begin{matrix}\phi: \mathbb{R} \to \mathbb{R}^4 &,& \phi(x) = \begin{bmatrix}1 \\ x \\ x^2 \\ x^3 \end{bmatrix}\end{matrix}$$
in
$$\theta_3 \cdot x^3 + \theta_2 \cdot x^2 + \theta_1 \cdot x + \theta_0 = \theta^T \cdot \phi(x)$$

![[2.4. Support Vector Machines - SVM 06-02-2023 10.29.34.excalidraw|1900]]

# Notations

- $y \in \{-1, 1\}$ instead of $\{0, 1\}$
- parameters $w$ and $b$ instead of the vector $\phi$
- the classifier $$h_{w, b}(x) = g(w^T \cdot x + b)$$
where $g(z) = 1$ if $z \ge 0$ and $g(z) = -1$ otherwise.

# Scalar product
$$\begin{pmatrix}a & b & c\end{pmatrix} \cdot \begin{pmatrix} a' \\ b' \\ c' \end{pmatrix} = < \begin{pmatrix}a & b & c\end{pmatrix}, \begin{pmatrix}a' & b' & c'\end{pmatrix}> = a \cdot a' + b \cdot b' + c \cdot b'$$
El producto escalar mide lo parecido que son dos vectores.
Dos vectores con norma 1, si su escalar es:
- +1 ==> son iguales
-  0 ==> son perpendiculares
- -1 ==> son opuestos

# Hyperplane
The equation $<x, w> = 0$ splits the pane in 2 regions:
- Positives = $\{x: <x, w> \ge 0\}$
- Negatives = $\{x: <x, w> < 0\}$

![[2.4. Support Vector Machines - SVM 06-02-2023 10.44.38.excalidraw|1900]]

## Affine hyperplane
Hyperplanes translated to a point $x_0$ in the space.
The equation $<x-x_0, w> = <x, w> + b = 0$ splits the pane in 2 regions:
- Positives = $\{x: <x-x_0, w> \ge 0\}$
- Negatives = $\{x: <x-x_0, w> < 0\}$

![[2.4. Support Vector Machines - SVM 06-02-2023 10.45.47.excalidraw|1900]]

## Distance to hyperplanes
if $||x|| = 1$ then $<x, y> = ||y|| \cdot cos(x, y)$

## Distance to affine hyperplanes
Distance from $x$ to the hyperplane $<w,x> + b = 0$
$$d(x, hyperplane) = { { <w, x> + b } \over {||w||}}$$
# Optimization of SVM
![[2.4. Support Vector Machines - SVM 06-02-2023 10.51.04.excalidraw|1900]]

Con el objetivo de separar las clases círculo y cuadrado, las 3 rectas cumplen, pero las 2 y 3 son menos 'claras' que la 1. Si buscamos maximizar el margen, se minimiza $||w||$.

## *Soft margin*
![[2.4. Support Vector Machines - SVM 06-02-2023 10.58.22.excalidraw|1900]]
Cuando las clases no son separables, el mejor margen se denomina *soft margin*.

Para optimizar el *soft margin*:
$$min_{w,\lambda, b} { 1 \over 2 } \cdot <w, w> + C \sum_{i=1}^m \lambda_i$$
$$\begin{matrix}s.t. & & y_i(<w, \phi(x_i)> + b) \ge 1 - \lambda_i,\end{matrix}$$
where
$$\begin{matrix}\lambda_i \ge 0 &,& i=1,...,m\end{matrix}$$
