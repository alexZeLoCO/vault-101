# 8. Linear Regression with gradient descent
2023-04-11 13:11 (YYYY-MM-DD) @ 13:12
Rodríguez López, Alejandro // UO281827

Tags:
	#showable
	Hecho en #EPI
	Sobre #Sistemas_Inteligentes 
	Para #Apuntes 
	Otros:
	Refs:
 
<hr>

Regresión / Clasificación
- Regresión: Algoritmo que predice valores de una función continua.
- Clasificación: Algoritmo que predice valores de una función multivariable.

Con la regresión lineal se busca una función que prediga un valor dadas unas variables de entrada $x_0,\ ...,\ x_k$.
$$
h_{\theta}(x) = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2
$$
o generalmente,
$$
h_\theta(x) = \theta_0 + \sum_{i=1}^{k}{(\theta_i \cdot x_i)}
$$
si se supone que $x_0 = 1$ (termino *intercept*), entonces se puede decir que
$$
h_\theta(x) = \sum_{i=0}^{k}{(\theta_i \cdot x_i)}
$$
que se suele abreviar como
$$
h_\theta(x) = \theta^T \cdot x
$$
Los datos $\theta_0,\ ...,\ \theta_k$ son pesos.

## Cost function
(Another one...)
$$
J(\theta) = {1 \over 2} \sum_{i=1}^{n}{(h_\theta(x_i) - y_i)^2}
$$
$$
J(\theta) = {\sum_{(x, y) \in D}{(h_\theta((x, y)) - Y(x, y))^2} \over |D|}
$$
where $|D|$ is the size of the set, not its absolute value.