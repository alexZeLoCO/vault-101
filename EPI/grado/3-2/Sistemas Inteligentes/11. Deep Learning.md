# 11. Deep Learning
<% tp.file.creation_date() %> (YYYY-MM-DD) @ 10:07
Rodríguez López, Alejandro // UO281827

Tags:
	#showable
	Hecho en #EPI
	Sobre #Sistemas_Inteligentes 
	Para #Apuntes 
	Otros:
	Refs:
	 [[5. Deep Learning]]
 
<hr>

Se parte de un conjunto de datos orígen $x$ y se quieren obtener unos resultados $y$. Tanto $x$ como $y$ son vectores.
Se busca una función $f(x)$ tal que $f(x_k) = y_k$. Esta función realiza operaciones matriciales entre $x$ y otras matrices $\theta$.
La calidad de la función $f_{\theta}(x)$ se calcula utilizando una función de pérdida.
$$
\theta \leftarrow argmin_\theta { 1 \over |D| } \sum_{(x, y) \in D } loss(y, f_{\theta}(x))
$$
Donde $loss(y, f_{\theta}(x))$ podría ser
$$
loss(y, f_{\theta}(x)) = { 1 \over 2 } \cdot (y - f_\theta(x)^2)
$$
En el mejor de los casos, la función de pérdida es de grado 2 y entonces su mínimo local existe y es igual al global. Si fuese de grado 1, sería una función monótona y no encontraríamos un valor determinado que sea mínimo.

## No linealidad

El hecho de que $f_{\theta}(x)$ sea no lineal resulta en grandes mejoras frente a si $f_{\theta}(x)$ fuese lineal.
Pero no podemos obtener la no linealidad multiplicando matrices.
Para ello utilizamos funciones como ReLU. $g(z) = max\{0, z\}$. Esta función no es lineal.

## Descenso del Gradiente

Si nuestro objetivo es minimizar la función de pérdida $loss(y, f_{\theta}(x))$ (a la que nos referiremos como $J$), entonces nos interesa conocer la derivada de la función, para conocer en qué dirección se encuentra el mínimo.

## General Framework

La distribución $Pr(y|x, \theta)$, contiene la función $f: R^m \rightarrow R^n, f_{\theta}(x)$.