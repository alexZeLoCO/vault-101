<% tp.file.creation_date() %> (YYYY-MM-DD) @ 11:26
Rodríguez López, Alejandro // UO281827

Tags:
	#showable
	Hecho en #EPI
	Sobre #Sistemas_Inteligentes 
	Para #Apuntes 
	Otros:
	Refs:
 
<hr>

Deep Learning es un algoritmo de aprendizaje automático.

# The goal of deep learning
Look for the nonlinear function:
$$f: \mathbb{R}^m \rightarrow \mathbb{R}^n, f(x, \theta) = f_\theta(x)$$
With the greatest consistency with the data
$$D = \{(x, y) : x \rightsquigarrow y\}$$
What is meaning of consistency in this context?
- Ease estimations $y \approx \Psi(f(x, \theta))$
- Make minimum a loss function (inconsistency with the data)

## Loss function
$$\theta \leftarrow argmin_\theta\ {1 \over {|D|}} \sum_{(x, y) \in D} loss(y, f_\theta(x))$$
Example: Linear regression $(n=1)$ with a quadratic loss
$$
\begin{matrix}
loss(y, f_w(x)) &=& { 1 \over 2 } \cdot (y - f_w(x))^2 \\
f_w(x) &=& w \cdot x
\end{matrix}
$$
	
<hr>

2023-02-20 (YYYY-MM-DD) @ 10:13

Las soluciones al problema son no lineales. ==> La función de pérdida es no convexa.

```functionplot
---
title: Función Convexa
xLabel: 
yLabel: 
bounds: [-0.5,1,-0.1,1]
disableZoom: false
grid: true
---
f(x) = x^2
```
```functionplot
---
title: Función NO Convexa
xLabel: 
yLabel: 
bounds: [-10,10,-1.5,1.5]
disableZoom: false
grid: true
---
f(x) = sin(x) 
```

Es más fácil resolver el mínimo de una función convexa que el de una no convexa.

# Optimización
La optimización requiere sus derivadas. Buscamos minimizar la función:
$$\theta \leftarrow argmin_\theta\ {1 \over {|D|}} \sum_{(x, y) \in D} loss(y, f_\theta(x))$$
Donde $y$ es el resultado real, $x$ es la entrada al modelo y $f_{\theta}(x)$ es lo que retorna el modelo. Lo que retorna el modelo se calcula en función de $\theta$.
La función a optimizar es multivariable, las derivadas parciales se deben realizar en función de $\theta$

## Gradient Descent
Reescribimos la funcion:

$$\theta \leftarrow argmin_\theta\ {1 \over {|D|}} \sum_{(x, y) \in D} loss(y, f_\theta(x)) = \theta \leftarrow argmin_\theta\ J(\theta)$$

$$
\begin{matrix}
\theta \leftarrow \theta_0 \\
repeat\\
&\theta \leftarrow \theta - \gamma \cdot { \partial J \over \partial \theta } \\
until\ no\ more\ improvement\ can\ be\ made\\
return\ \theta
\end{matrix}
$$

# General framework
$$D = \{(x,y): x \rightsquigarrow y\}$$
We learn a distribution
$$Pr(y|x, \theta)$$
That involves the function
$$f: \mathbb{R}^m \rightarrow \mathbb{R}^n, f_\theta(x)$$
This requires the maximum likelihood
$$argmax_\theta \prod_D Pr(x|y,\theta)$$
O lo que es lo mismo:
$$argmin_\theta -log(\prod_D Pr(y|x,\theta)) = argmin_\theta - \sum_D log (Pr(y|x,\theta))$$
En muchos casos la operación anterior se resolverá a algo similar a: $log(e^x)$ que se resuelve trivialmente a $x$.

# Regresión
$$
\begin{matrix}
y \in \mathbb{R};\ D &=& \{(x, y),\ ...\} \\
Pr(y|x, \theta) &=& N(y;f(x, \theta), \sigma^2) \\
N(y;f(x, \theta), \sigma^2) &=& \sqrt{1 \over {2\cdot \pi \cdot \sigma^2}} \cdot e^{-{1 \over {2 \cdot \sigma^2}}\cdot (y-\mu)^2}\\
-log(Pr(y|x,\theta)) &=& log (\sigma) + {1 \over 2} \cdot log(2 \cdot \pi) + {1 \over {2 \cdot \sigma^2}} \cdot (y - f(x, \theta))^2 \\
argmin_\theta - log( \prod_D (Pr(y|x,\theta))) &=& argmin_\theta \sum_D (y-f(x, \theta))^2
\end{matrix}
$$

$$
Pr(y|x, \theta) \propto e^{y \cdot f(x, \theta)}
$$
Entonces,
$$
\begin{matrix}
Pr(y|x,\theta) &=& {{e^{y\cdot f(x, \theta)}}\over{\sum_{y'}e^{y' \cdot f(x, \theta)}}} \\
&=&{e^{y \cdot f(x, \theta)} \over {1+e^{f(x, \theta)}}}
\end{matrix}
$$

<hr>

2023-03-21 (YYYY-MM-DD) @ 11:50

## PL T4: Regresión Keras
	[Regresión Keras](https://colab.research.google.com/drive/1jkYmlkgEn7sndoH2P8qw7j7_oYNfyeRs#scrollTo=R9tXs9SBfnr_)
	
### Modelo Lineal
$$
\begin{matrix}
	\begin{bmatrix}
		x_0 & x_1
	\end{bmatrix}
	\times
	\begin{bmatrix}
		w_0 \\
		w_1
	\end{bmatrix}
	+ bias = y
\end{matrix}
$$

Tras calcular $y$, se aplica la función de pérdida ( $loss(y,\ y')$ )y se modifica la matriz $w$ y el valor $bias$ para reducir la pérdida.
```
=================================================================
 Layer (type)                Output Shape              Param #   
=================================================================
 output_layer (Dense)        (None, 1)                 3         
                                                                 
=================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
_________________________________________________________________
```
Param # es el número de parámetros que el modelo puede modificar para obtener $y$ a partir de $x$. En este caso:
- $w_0$
- $w_1$
- $bias$

Parámetro: Valores que controla el modelo para obtener el resultado.
Hiperparámetro: Valores que controla el desarrollador para modificar el comportamiento del modelo.
- n_epochs
- learning_rate
- batch -> En lugar de pasar una entrada de $x$ al modelo, se le pasan varias.
$$
\begin{bmatrix}
	x_{00} & x_{01} \\
	x_{10} & x_{11} \\
	x_{20} & x_{21} \\
	... & ... \\
	x_{batch0} & x_{batch1} \\
\end{bmatrix}
$$
Al aumentar el número de datos, los pesos se actualizan menos veces.

Respecto al problema $y = \sin(x_0) + \cos(x_1)$, si se aplica un modelo lineal, la función $loss(y, y')$ tenderá a $1$, que es $1 \over 4$ de la amplitud del intervalo en el que existen valores de $\sin(x_0) + \cos(x_1)$.

### Modelo no Lineal
Se aplican funciones de activación.
- ReLu: 
```py
def relu (x) -> int:
	return x if x > else 0
```
- sigmoid

Se deben tener precauciones porque las funciones de activación eliminan valores y pueden llevar a error.
Para solucionar este problema, se añade una capa Dense al modelo.

``` mermaid
graph LR;
	INPUT --> DENSE_0
	subgraph MODEL
	subgraph RELU
	DENSE_0 --> RELU_0
	end
	RELU_0 --> DENSE_1
	end
	DENSE_1 --> OUTPUT
```
